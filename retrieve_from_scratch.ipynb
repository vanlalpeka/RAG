{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a simple RAG implementation using ollama LLM.\n",
    "\n",
    "Local (context) dataset is sliced or \"chunked\", and the chunks are vectorized using ollama's embedding model. The retrieval function takes a query and returns the top N relevant chunks using cosine similarity. We also have instruction_prompt that acts as a guardrail to \"focus\" ollama's response. \n",
    "\n",
    "The user input is used to retrieve from the vectorized database. This, along with the instruction_prompt, is used to call ollama's chat() function and get a response.\n",
    "***\n",
    "\n",
    "Areas for improvement:\n",
    "1. Replace cosine similarity with more robust and context-aware function to retrieve the vectors.\n",
    "2. Change VECTOR_DB storage location for scale.\n",
    "3. Different method of chunking as opposed to having each line as a chunk, as we have it here.\n",
    "\n",
    "***\n",
    "\n",
    "Pre-work:\n",
    "\n",
    "    1. Install ollama CLI: \n",
    "\n",
    "    ```\n",
    "    curl -fsSL https://ollama.com/install.sh | sh\n",
    "    ```\n",
    "\n",
    "    2. Download the embedding model and the language model from Hugging Face:\n",
    "    ```\n",
    "    ollama pull hf.co/CompendiumLabs/bge-base-en-v1.5-gguf\n",
    "    ollama pull hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF\n",
    "    ```\n",
    "\n",
    "    3. Install ollama python package:\n",
    "    ```\n",
    "    pip install ollama\n",
    "    ```\n",
    "\n",
    "    4. Download the dataset:\n",
    "\n",
    "    ```\n",
    "    cd datasets && curl -O https://huggingface.co/ngxson/demo_simple_rag_py/blob/main/cat-facts.txt && cd ..\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "with open('datasets/cat-facts.txt', 'r') as file:\n",
    "  dataset = file.readlines()\n",
    "  print(f'Loaded {len(dataset)} entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the vector database\n",
    "\n",
    "- The embedding model from ollama converts chunks to embedding vector. \n",
    "- The chunks and the corresponding vectors are then stored in a list.\n",
    "- This notebook/example considers each line in the dataset as a chunk, for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 1/150 to the database\n",
      "Added chunk 2/150 to the database\n",
      "Added chunk 3/150 to the database\n",
      "Added chunk 4/150 to the database\n",
      "Added chunk 5/150 to the database\n",
      "Added chunk 6/150 to the database\n",
      "Added chunk 7/150 to the database\n",
      "Added chunk 8/150 to the database\n",
      "Added chunk 9/150 to the database\n",
      "Added chunk 10/150 to the database\n",
      "Added chunk 11/150 to the database\n",
      "Added chunk 12/150 to the database\n",
      "Added chunk 13/150 to the database\n",
      "Added chunk 14/150 to the database\n",
      "Added chunk 15/150 to the database\n",
      "Added chunk 16/150 to the database\n",
      "Added chunk 17/150 to the database\n",
      "Added chunk 18/150 to the database\n",
      "Added chunk 19/150 to the database\n",
      "Added chunk 20/150 to the database\n",
      "Added chunk 21/150 to the database\n",
      "Added chunk 22/150 to the database\n",
      "Added chunk 23/150 to the database\n",
      "Added chunk 24/150 to the database\n",
      "Added chunk 25/150 to the database\n",
      "Added chunk 26/150 to the database\n",
      "Added chunk 27/150 to the database\n",
      "Added chunk 28/150 to the database\n",
      "Added chunk 29/150 to the database\n",
      "Added chunk 30/150 to the database\n",
      "Added chunk 31/150 to the database\n",
      "Added chunk 32/150 to the database\n",
      "Added chunk 33/150 to the database\n",
      "Added chunk 34/150 to the database\n",
      "Added chunk 35/150 to the database\n",
      "Added chunk 36/150 to the database\n",
      "Added chunk 37/150 to the database\n",
      "Added chunk 38/150 to the database\n",
      "Added chunk 39/150 to the database\n",
      "Added chunk 40/150 to the database\n",
      "Added chunk 41/150 to the database\n",
      "Added chunk 42/150 to the database\n",
      "Added chunk 43/150 to the database\n",
      "Added chunk 44/150 to the database\n",
      "Added chunk 45/150 to the database\n",
      "Added chunk 46/150 to the database\n",
      "Added chunk 47/150 to the database\n",
      "Added chunk 48/150 to the database\n",
      "Added chunk 49/150 to the database\n",
      "Added chunk 50/150 to the database\n",
      "Added chunk 51/150 to the database\n",
      "Added chunk 52/150 to the database\n",
      "Added chunk 53/150 to the database\n",
      "Added chunk 54/150 to the database\n",
      "Added chunk 55/150 to the database\n",
      "Added chunk 56/150 to the database\n",
      "Added chunk 57/150 to the database\n",
      "Added chunk 58/150 to the database\n",
      "Added chunk 59/150 to the database\n",
      "Added chunk 60/150 to the database\n",
      "Added chunk 61/150 to the database\n",
      "Added chunk 62/150 to the database\n",
      "Added chunk 63/150 to the database\n",
      "Added chunk 64/150 to the database\n",
      "Added chunk 65/150 to the database\n",
      "Added chunk 66/150 to the database\n",
      "Added chunk 67/150 to the database\n",
      "Added chunk 68/150 to the database\n",
      "Added chunk 69/150 to the database\n",
      "Added chunk 70/150 to the database\n",
      "Added chunk 71/150 to the database\n",
      "Added chunk 72/150 to the database\n",
      "Added chunk 73/150 to the database\n",
      "Added chunk 74/150 to the database\n",
      "Added chunk 75/150 to the database\n",
      "Added chunk 76/150 to the database\n",
      "Added chunk 77/150 to the database\n",
      "Added chunk 78/150 to the database\n",
      "Added chunk 79/150 to the database\n",
      "Added chunk 80/150 to the database\n",
      "Added chunk 81/150 to the database\n",
      "Added chunk 82/150 to the database\n",
      "Added chunk 83/150 to the database\n",
      "Added chunk 84/150 to the database\n",
      "Added chunk 85/150 to the database\n",
      "Added chunk 86/150 to the database\n",
      "Added chunk 87/150 to the database\n",
      "Added chunk 88/150 to the database\n",
      "Added chunk 89/150 to the database\n",
      "Added chunk 90/150 to the database\n",
      "Added chunk 91/150 to the database\n",
      "Added chunk 92/150 to the database\n",
      "Added chunk 93/150 to the database\n",
      "Added chunk 94/150 to the database\n",
      "Added chunk 95/150 to the database\n",
      "Added chunk 96/150 to the database\n",
      "Added chunk 97/150 to the database\n",
      "Added chunk 98/150 to the database\n",
      "Added chunk 99/150 to the database\n",
      "Added chunk 100/150 to the database\n",
      "Added chunk 101/150 to the database\n",
      "Added chunk 102/150 to the database\n",
      "Added chunk 103/150 to the database\n",
      "Added chunk 104/150 to the database\n",
      "Added chunk 105/150 to the database\n",
      "Added chunk 106/150 to the database\n",
      "Added chunk 107/150 to the database\n",
      "Added chunk 108/150 to the database\n",
      "Added chunk 109/150 to the database\n",
      "Added chunk 110/150 to the database\n",
      "Added chunk 111/150 to the database\n",
      "Added chunk 112/150 to the database\n",
      "Added chunk 113/150 to the database\n",
      "Added chunk 114/150 to the database\n",
      "Added chunk 115/150 to the database\n",
      "Added chunk 116/150 to the database\n",
      "Added chunk 117/150 to the database\n",
      "Added chunk 118/150 to the database\n",
      "Added chunk 119/150 to the database\n",
      "Added chunk 120/150 to the database\n",
      "Added chunk 121/150 to the database\n",
      "Added chunk 122/150 to the database\n",
      "Added chunk 123/150 to the database\n",
      "Added chunk 124/150 to the database\n",
      "Added chunk 125/150 to the database\n",
      "Added chunk 126/150 to the database\n",
      "Added chunk 127/150 to the database\n",
      "Added chunk 128/150 to the database\n",
      "Added chunk 129/150 to the database\n",
      "Added chunk 130/150 to the database\n",
      "Added chunk 131/150 to the database\n",
      "Added chunk 132/150 to the database\n",
      "Added chunk 133/150 to the database\n",
      "Added chunk 134/150 to the database\n",
      "Added chunk 135/150 to the database\n",
      "Added chunk 136/150 to the database\n",
      "Added chunk 137/150 to the database\n",
      "Added chunk 138/150 to the database\n",
      "Added chunk 139/150 to the database\n",
      "Added chunk 140/150 to the database\n",
      "Added chunk 141/150 to the database\n",
      "Added chunk 142/150 to the database\n",
      "Added chunk 143/150 to the database\n",
      "Added chunk 144/150 to the database\n",
      "Added chunk 145/150 to the database\n",
      "Added chunk 146/150 to the database\n",
      "Added chunk 147/150 to the database\n",
      "Added chunk 148/150 to the database\n",
      "Added chunk 149/150 to the database\n",
      "Added chunk 150/150 to the database\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "\n",
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))\n",
    "\n",
    "for i, chunk in enumerate(dataset):\n",
    "  add_chunk_to_database(chunk)\n",
    "  print(f'Added chunk {i+1}/{len(dataset)} to the database')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The retrieval function\n",
    "The retrieval function takes a query or a prompt and returns the top N most relevant chunks based on a selected distance function i.e. 'cosine similarity' in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def retrieve(query, top_n=3):\n",
    "  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "  # temporary list to store (chunk, similarity) pairs\n",
    "  similarities = []\n",
    "  for chunk, embedding in VECTOR_DB:\n",
    "    similarity = cosine_similarity(query_embedding, embedding)\n",
    "    similarities.append((chunk, similarity))\n",
    "  # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  # finally, return the top N most relevant chunks\n",
    "  return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "Two key parameters for output generation are input query and instruction prompt. The first is what the user entered in the chat, and the latter acts as a 'guardrail' to set the context to the local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      " - (similarity: 0.69) The color of the points in Siamese cats is heat related. Cool areas are darker.\n",
      "\n",
      " - (similarity: 0.68) A form of AIDS exists in cats.\n",
      "\n",
      " - (similarity: 0.68) Two members of the cat family are distinct from all others: the clouded leopard and the cheetah. The clouded leopard does not roar like other big cats, nor does it groom or rest like small cats. The cheetah is unique because it is a running cat; all others are leaping cats. They are leaping cats because they slowly stalk their prey and then leap on it.\n",
      "\n",
      "You are a helpful chatbot.\n",
      "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
      " - The color of the points in Siamese cats is heat related. Cool areas are darker.\n",
      "\n",
      " - A form of AIDS exists in cats.\n",
      "\n",
      " - Two members of the cat family are distinct from all others: the clouded leopard and the cheetah. The clouded leopard does not roar like other big cats, nor does it groom or rest like small cats. The cheetah is unique because it is a running cat; all others are leaping cats. They are leaping cats because they slowly stalk their prey and then leap on it.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input query\n",
    "\n",
    "# input_query = input('Ask me a question: ')\n",
    "input_query = 'tell me about the peculiarities of cats'\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "  print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "line_break = '\\n'\n",
    "instruction_prompt = f'''You are a helpful chatbot.\n",
    "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
    "{line_break.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}\n",
    "'''\n",
    "\n",
    "print(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "The curious case of feline characteristics! As you mentioned, cool areas appear darker in Siamese cats due to a heat-related phenomenon, which is an interesting aspect of their behavior. This unique trait can be attributed to the fact that cooler skin absorbs more light, making them appear darker.\n",
      "\n",
      "As for two members of the cat family being distinct from all others: the clouded leopard and the cheetah - it's actually the opposite! The cheetah is a running cat because they don't stalk prey like other big cats, but leap on their prey instead. And while the clouded leopard does groom and rest like small cats, there are no members of this family that \"roar\" or engage in leisurely activities like grooming.\n",
      "\n",
      "The clouded leopard is actually one of the most agile and flexible cats in the world, with a slender build and incredibly short legs. They're well-suited for their arboreal lifestyle, spending much of their time in trees. The cheetah, on the other hand, has long legs that enable it to run at incredible speeds, making it a formidable hunter.\n",
      "\n",
      "These physical characteristics highlight just how unique each species within the feline family is!"
     ]
    }
   ],
   "source": [
    "# call Ollama's chat function with the instruction prompt and the input query\n",
    "# the chat function will return a generator that will yield the chatbot's responses\n",
    "# one by one, as they are generated\n",
    "# the stream=True argument is used to enable real-time responses\n",
    "# the LANGUAGE_MODEL is the model that will be used to generate the responses\n",
    "\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "# print the response from the chatbot in real-time\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ragvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
