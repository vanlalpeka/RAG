{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain langchain-huggingface bs4 langsmith langchain_ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, let me tackle this query step by step. The user is asking about Task Decomposition, specifically wanting an explanation in three sentences. They've provided a context that defines what Task Decomposition is and even includes some technical details.\n",
      "\n",
      "First, I need to ensure I fully understand the main points of Task Decomposition. From the context, it seems like it's a technique where complex tasks are broken down into simpler parts for easier handling by AI models like LLMs. The technique helps models think step-by-step, improving performance on hard tasks by enhancing test-time computation.\n",
      "\n",
      "Looking at the context again, I see that there's information about different methods such as CoT (Chain of Thought) and its extensions. YAO et al.'s work discusses this in depth. So while the user's main question is straightforward, the context provides additional insights into various techniques for decomposition.\n",
      "\n",
      "I need to condense this information without missing any key points. The answer should define Task Decomposition, mention common methods like CoT, the advancements with tree-of-thought, and specific applications like step-by-step instructions or human interventions.\n",
      "\n",
      "Wait, does the user want a brief definition that's clear but concise? They also specified up to three sentences, so I shouldn't go into too much detail. It's important to strike a balance between explaining what it is and giving enough technical context about why it's useful.\n",
      "\n",
      "Let me make sure I include all necessary points without exceeding the sentence limit. The key areas are the step-wise thinking by LLMs, various methods like CoT and tree-of-thought, their applications, and its role in enhancing model performance.\n",
      "\n",
      "Yes, that should cover everything succinctly.\n",
      "</think>\n",
      "\n",
      "Task decomposition is a technique where complex tasks are broken down into simpler, manageable parts to improve model performance on hard tasks. It involves instructing models to \"think step by step\" or using prompts like \"Steps for XYZ.\" Various methods, including CoT (Chain of Thought) and tree-of-thought, enhance decomposability by exploring multiple reasoning possibilities at each step.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# llm = Ollama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "# prompt = \"\"\"\n",
    "# Use ONLY the context below.\n",
    "# If unsure, say \"I don't know\".\n",
    "# Keep answers under 4 sentences.\n",
    "\n",
    "# Context: {context}\n",
    "# Question: {question}\n",
    "# Answer:\n",
    "# \"\"\"\n",
    "# QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Document\n",
    "# This document will be split into chunks -> vector embed -> added to the the prompt for context\n",
    "# WebBaseLoader() standardizes webpage document, and transform them into a Document structure with two main fields: page_content (raw text) and metadata (additional information)\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed. Create vector embedding for each chunk. Chroma runs locally.\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    # embedding=OpenAIEmbeddings())   # OpenAI almost always have better embedding performance compared to open-source counterparts\n",
    "                                    embedding=OllamaEmbeddings(model=\"mxbai-embed-large\"))  # SOTA (as of March 2024) large embedding model from mixedbread.ai\n",
    "\n",
    "# The vector embedding that will be added to the prompt, for context\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt for RAG\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# The default value of temperature is 1.0.\n",
    "# USE CASE\tTEMPERATURE\n",
    "# Coding / Math   \t0.0\n",
    "# Data Cleaning / Data Analysis\t1.0\n",
    "# General Conversation\t1.3\n",
    "# Translation\t1.3\n",
    "# Creative Writing / Poetry\t1.5\n",
    "llm = OllamaLLM(model=\"deepseek-r1:1.5b\", temperature=1.3)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain. \n",
    "# Takes the input question \n",
    "# -> run the retriever to fetch all the documents \n",
    "# -> put the question and the retrieved documents into the prompt \n",
    "# -> pass it to the LLM \n",
    "# -> format the output as a string \n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "print(rag_chain.invoke(\"What is Task Decomposition?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "We have (external) documents that we want to load to a 'retriever' (as in the code above). The goal of the retriever is to return relevant chunks from the documents based on the user's question. The relevance of the chunks can be calculate in multiple ways. For example:\n",
    "- Statistical - Bag of word, sparse representation, search: BM25 \n",
    "- Machine Learning - vector embedding, dense representation, search: KNN, HNSW\n",
    "\n",
    "To rephrase, the documents are segmented into chunks, which are vector embedded and indexed. The question is also vector embedded. This is followed with some similarity calculation between the question embedding and the indexed document chunks to filter the relevant chunks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
